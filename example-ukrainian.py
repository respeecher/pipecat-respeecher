#
# Copyright (c) 2024‚Äì2025, Daily
# Copyright (c) 2025, Respeecher
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Respeecher Quickstart Example.

The example runs a simple voice AI bot that you can connect to using your
browser and speak with it. You can also deploy this bot to Pipecat Cloud.

Required AI services:
- Deepgram (Speech-to-Text)
- Google or Cerebras (LLM)
- Respeecher (Text-to-Speech)

Run the bot using::

    uv run bot.py
"""

import os

from dotenv import load_dotenv
from loguru import logger

print("üöÄ Starting Pipecat bot...")
print("‚è≥ Loading models and imports (20 seconds, first run only)\n")

logger.info("Loading Local Smart Turn Analyzer V3...")
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3

logger.info("‚úÖ Local Smart Turn Analyzer V3 loaded")
logger.info("Loading Silero VAD model...")
from pipecat.audio.vad.silero import SileroVADAnalyzer

logger.info("‚úÖ Silero VAD model loaded")

from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import LLMRunFrame

logger.info("Loading pipeline components...")
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import (
    LLMContextAggregatorPair,
    LLMUserAggregatorParams,
)
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.transcriptions.language import Language
from pipecat.services.deepgram.stt import DeepgramSTTService, LiveOptions
from pipecat.services.cerebras.llm import CerebrasLLMService
from pipecat.services.google.llm import GoogleLLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams
from pipecat.turns.user_stop.turn_analyzer_user_turn_stop_strategy import (
    TurnAnalyzerUserTurnStopStrategy,
)
from pipecat.turns.user_turn_strategies import UserTurnStrategies
from pipecat_respeecher import RespeecherTTSService
from pipecat_whisker import WhiskerObserver

logger.info("‚úÖ All components loaded successfully!")

load_dotenv(override=True)


async def run_bot(transport: BaseTransport, runner_args: RunnerArguments):
    logger.info("Starting bot")

    stt = DeepgramSTTService(
        api_key=os.getenv("DEEPGRAM_API_KEY"),
        live_options=LiveOptions(language=Language.UK),
    )

    tts = RespeecherTTSService(
        api_key=os.getenv("RESPEECHER_API_KEY"),
        voice_id="olesia-conversation",
        model="public/tts/ua-rt",
        # [Optional] Sampling parameters overrides.
        # Can be changed on the fly with TTSUpdateSettingsFrame,
        # just like the model and the voice.
        params=RespeecherTTSService.InputParams(
            sampling_params={
                "min_p": 0.01,
            },
        ),
    )

    cerebras_api_key = os.getenv("CEREBRAS_API_KEY")
    google_api_key = os.getenv("GOOGLE_API_KEY")

    if cerebras_api_key:
        llm = CerebrasLLMService(api_key=cerebras_api_key, model="llama3.1-8b")
    elif google_api_key:
        llm = GoogleLLMService(api_key=google_api_key)
    else:
        raise ValueError("Neither Google nor Cerebras API key is provided")

    messages = [
        {
            "role": "system",
            "content": "–¢–∏ –¥—Ä—É–∂–Ω—ñ–π –®–Ü –∞—Å–∏—Å—Ç–µ–Ω—Ç, —â–æ —Ä–æ–∑–º–æ–≤–ª—è—î —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –ø—Ä–∏–≤—ñ—Ç–Ω–æ —Ç–∞ –ø—ñ–¥—Ç—Ä–∏–º—É–π —Ä–æ–∑–º–æ–≤—É. –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –µ–º–æ–¥–∑—ñ —Ç–∞ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Å–∏–º–≤–æ–ª–∏ —É —Å–≤–æ—ó—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥—è—Ö. –¢–∏ –∂—ñ–Ω–æ—á–æ–≥–æ —Ä–æ–¥—É.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(
        context,
        user_params=LLMUserAggregatorParams(
            user_turn_strategies=UserTurnStrategies(
                stop=[
                    TurnAnalyzerUserTurnStopStrategy(
                        turn_analyzer=LocalSmartTurnAnalyzerV3()
                    )
                ]
            ),
        ),
    )

    # [Optional] Without RTVI, the chat interface in the WebRTC demo page won't work.
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            transport.input(),  # Transport user input
            rtvi,
            stt,
            context_aggregator.user(),  # User responses
            llm,  # LLM
            tts,  # TTS
            transport.output(),  # Transport bot output
            context_aggregator.assistant(),  # Assistant spoken responses
        ]
    )

    # [Optional] Whisker is a Pipecat debugger/visualizer.
    # https://github.com/pipecat-ai/whisker
    whisker = WhiskerObserver(pipeline)

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
            audio_out_sample_rate=22050,
        ),
        idle_timeout_secs=runner_args.pipeline_idle_timeout_secs,
        observers=[RTVIObserver(rtvi), whisker],
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")
        messages.append(
            {"role": "system", "content": "–ü—Ä–∏–≤—ñ—Ç–∞–π—Å—è —Ç–∞ –∫–æ—Ä–æ—Ç–∫–æ —Ä–æ–∑–∫–∞–∂–∏ –ø—Ä–æ —Å–µ–±–µ."}
        )
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=runner_args.handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point for the bot starter."""

    transport_params = {
        "daily": lambda: DailyParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        ),
        "webrtc": lambda: TransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        ),
    }

    transport = await create_transport(runner_args, transport_params)

    await run_bot(transport, runner_args)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()
